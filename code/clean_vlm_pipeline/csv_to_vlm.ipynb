{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Format and Upload CSV Files (if using Colab)\n",
        "\n",
        "The CSV file must have the following two columns in order for processing to work.\n",
        "\n",
        "'youtube_id' - 11-character code that is unique to each video on YouTube\n",
        "\n",
        "'start_seconds' - a floating point number representing the elapsed seconds at the start of the clip"
      ],
      "metadata": {
        "id": "s3jaCoZ2ktbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specify the Input CSV File Path\n"
      ],
      "metadata": {
        "id": "LE1cL0W9lwBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"\""
      ],
      "metadata": {
        "id": "u2AyjgPRlr82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and Import Necessary Python Packages"
      ],
      "metadata": {
        "id": "Xbnv_ARytT5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install katna\n",
        "!pip install yt_dlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoSwPQtiM5iH",
        "outputId": "76545a3d-f131-4ff6-dc2e-03934cc8427f",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting katna\n",
            "  Downloading katna-0.9.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from katna) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from katna) (1.5.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from katna) (0.24.0)\n",
            "Requirement already satisfied: opencv-contrib-python>=3.4.7 in /usr/local/lib/python3.10/dist-packages (from katna) (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from katna) (1.26.4)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from katna) (0.5.1)\n",
            "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (from katna) (0.5.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from katna) (2.32.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from katna) (5.9.5)\n",
            "Collecting ffmpy (from katna)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->katna) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->katna) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->katna) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->katna) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->katna) (2024.8.30)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->katna) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->katna) (11.0.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->katna) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->katna) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->katna) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->katna) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->katna) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->katna) (3.5.0)\n",
            "Downloading katna-0.9.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: ffmpy, katna\n",
            "Successfully installed ffmpy-0.4.0 katna-0.9.2\n",
            "Collecting yt_dlp\n",
            "  Downloading yt_dlp-2024.12.13-py3-none-any.whl.metadata (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.1/172.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yt_dlp-2024.12.13-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt_dlp\n",
            "Successfully installed yt_dlp-2024.12.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from yt_dlp import YoutubeDL\n",
        "from bs4 import BeautifulSoup\n",
        "from Katna.video import Video\n",
        "from Katna.writer import KeyFrameDiskWriter\n",
        "import pandas as pd\n",
        "import json\n",
        "import subprocess\n",
        "import re\n",
        "import os"
      ],
      "metadata": {
        "id": "qDsk89IIeQuX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "15h_P5wll3Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_timestamps(df):\n",
        "  \"\"\"\n",
        "  Gets the starting timestamp of each Youtube video in a datafram\n",
        "\n",
        "  Args:\n",
        "      df (pandas Dataframe): Dataframe containing at 'youtube_id' and\n",
        "      'start_seconds' columns\n",
        "\n",
        "  Returns:\n",
        "      dict: Maps Youtube IDs to their starting timestamp\n",
        "  \"\"\"\n",
        "  return {row['youtube_id']: row['start_seconds'] for i, row in df.iterrows()}\n",
        "\n",
        "def clean_data_labels(data_path, label_json_path, output_path = None):\n",
        "  \"\"\"\n",
        "  Replace Audioset labels with human-readable form\n",
        "\n",
        "  Args:\n",
        "      data_path (str): Path to csv containing audioset data\n",
        "      label_json_path (str): Path to json file containing map from audioset\n",
        "      labels to human readable labels\n",
        "      output_path (str): Default is None. If specified, the cleaned CSV will be\n",
        "      written to the specified output path\n",
        "\n",
        "  Returns:\n",
        "      str: Extracted YouTube ID\n",
        "  \"\"\"\n",
        "  def cleaning(label):\n",
        "    label = label.replace('\"', '').replace(' ', '')\n",
        "    return ';'.join([id_label_map[i] for i in label.split(';')])\n",
        "\n",
        "  df = pd.read_csv(data_path)\n",
        "\n",
        "  with open(label_json_path, 'r') as file:\n",
        "      data = json.load(file)\n",
        "\n",
        "  id_label_map = {dictionary['id'] : dictionary['name'] for dictionary in data}\n",
        "\n",
        "  df['positive_labels'] = df[' positive_labels'].apply(cleaning)\n",
        "\n",
        "  if output_path:\n",
        "    df.to_csv(output_path)\n",
        "  return df\n",
        "\n",
        "def mkdir(name):\n",
        "    \"\"\"\n",
        "    Creates a directory using mkdir command line command\n",
        "\n",
        "    Args:\n",
        "        name (str): Name of directory to be created\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    subprocess.run(['mkdir', name])\n",
        "\n",
        "def extract_youtube_ids(filename):\n",
        "    \"\"\"\n",
        "    Extracts YouTube video ID from a filename\n",
        "\n",
        "    Args:\n",
        "        filenames (str): Filenames containing YouTube ID.\n",
        "\n",
        "    Returns:\n",
        "        str: Extracted YouTube ID\n",
        "    \"\"\"\n",
        "    pattern = r'\\[([-_\\w]{11})\\]'  # Matches square brackets with 11-character YouTube IDs\n",
        "\n",
        "    match = re.search(pattern, filename)\n",
        "    if match:\n",
        "      return (match.group(1))\n",
        "    return ''"
      ],
      "metadata": {
        "id": "BaezpIiPfRv2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code for Audio and Keyframe Extraction"
      ],
      "metadata": {
        "id": "ZOMLBANCl71E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUTUBE_URL_PREFIX = \"https://www.youtube.com/watch?v=\"\n",
        "\n",
        "def download_video(video_url,i):\n",
        "    \"\"\"\n",
        "    Downloads a youtube video to ith download folder using YoutubeDL\n",
        "\n",
        "    Args:\n",
        "        video_url (str): URL to video to download\n",
        "        i (int): Batch number\n",
        "\n",
        "    \"\"\"\n",
        "    opts = {'paths': {'home': f'downloads{i}'}}\n",
        "    with YoutubeDL(opts) as yt:\n",
        "        yt.download(video_url)"
      ],
      "metadata": {
        "id": "yHAzfWfGLboy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_column(yt_ids, i, stop=None):\n",
        "    \"\"\"\n",
        "    Downloads a batch of YouTube videos based on their IDs.\n",
        "\n",
        "    Args:\n",
        "        yt_ids (list): List of YouTube video IDs to download.\n",
        "        i (int): Batch number for organizing downloads.\n",
        "        stop (int, optional): Number of videos to download. If None, downloads all videos.\n",
        "\n",
        "    Returns:\n",
        "        int: The number of videos that failed to download.\n",
        "    \"\"\"\n",
        "\n",
        "    # If stop is None, set it to the length of yt_ids\n",
        "    stop = len(yt_ids) if stop is None else stop\n",
        "\n",
        "    # Initialize a counter for failed downloads\n",
        "    sum = 0\n",
        "\n",
        "    # Iterate through the specified number of items\n",
        "    for yt_id in yt_ids[:stop]:\n",
        "        try:\n",
        "            # Attempt to download the video\n",
        "            download_video(YOUTUBE_URL_PREFIX + yt_id, i)\n",
        "        except Exception as e:\n",
        "            # Increment the failure counter on exception and continue\n",
        "            sum += 1\n",
        "            continue\n",
        "\n",
        "    # Return the total number of failed downloads\n",
        "    return sum"
      ],
      "metadata": {
        "id": "FDwgH5elPad1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_audio(id):\n",
        "    \"\"\"\n",
        "    Extracts audio from video files in the specified download folder and saves them as .wav files.\n",
        "\n",
        "    Args:\n",
        "        id (int): Batch number corresponding to the download folder.\n",
        "    \"\"\"\n",
        "\n",
        "    # Iterate through all files in the downloads folder for the given batch ID\n",
        "    for filename in os.listdir(f'downloads{id}'):\n",
        "        # Split the filename into the base name (root) and extension\n",
        "        root, extension = os.path.splitext(filename)\n",
        "\n",
        "        # Define the FFmpeg command to extract audio from the video file\n",
        "        command = [\n",
        "            'ffmpeg',\n",
        "            '-i', f'downloads{id}/{filename}',  # Input file\n",
        "            '-q:a', '0',  # Audio quality: highest quality (0)\n",
        "            '-map', 'a',  # Map only the audio streams\n",
        "            f'audio/{root}.wav'  # Output file path\n",
        "        ]\n",
        "\n",
        "        # Run the FFmpeg command\n",
        "        subprocess.run(command)\n"
      ],
      "metadata": {
        "id": "ON8qrvYJ6mRT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_clips(yt_ids, id):\n",
        "    \"\"\"\n",
        "    Trims video files to create clips based on predefined timestamps and saves them to the same folder.\n",
        "\n",
        "    Args:\n",
        "        yt_ids (list): List of YouTube video IDs corresponding to the videos.\n",
        "        id (int): Batch number corresponding to the download folder.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get a list of all filenames in the downloads folder for the given batch ID\n",
        "    filenames = [file for file in os.listdir(f'downloads{id}')]\n",
        "\n",
        "    # Map each filename to its YouTube ID using a helper function\n",
        "    title_to_id = {filename: extract_youtube_ids(filename) for filename in filenames}\n",
        "\n",
        "    # Process each file in the folder\n",
        "    for filename in filenames:\n",
        "        # Split the filename into the base name (root) and extension\n",
        "        root, extension = os.path.splitext(filename)\n",
        "\n",
        "        # Get the YouTube ID corresponding to the filename\n",
        "        video_id = title_to_id[filename]\n",
        "\n",
        "        # Construct the FFmpeg command to trim the video based on predefined timestamps\n",
        "        command = [\n",
        "            'ffmpeg',\n",
        "            '-i', f'downloads{id}/{filename}',  # Input file path\n",
        "            '-ss', str(timestamps[video_id]),  # Start time of the clip\n",
        "            '-to', str(timestamps[video_id] + 10),  # End time of the clip (10 seconds duration)\n",
        "            '-c', 'copy',  # Copy codec for faster processing\n",
        "            f'downloads{id}/{root}_cut{extension}'  # Output file path\n",
        "        ]\n",
        "\n",
        "        # Execute the FFmpeg command\n",
        "        subprocess.run(command)\n",
        "\n",
        "        # Remove the original video file after the trimmed clip is created\n",
        "        os.remove(f\"downloads{id}/{filename}\")\n"
      ],
      "metadata": {
        "id": "5EL9j__PinW2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_frames(id):\n",
        "    \"\"\"\n",
        "    Extracts keyframes from video files in the specified download folder and saves them to a frame directory.\n",
        "\n",
        "    Args:\n",
        "        id (int): Batch number corresponding to the download folder.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize the Video object for processing keyframes\n",
        "    vd = Video()\n",
        "\n",
        "    # Number of keyframes to extract from each video\n",
        "    no_of_frames_to_returned = 1\n",
        "\n",
        "    # Initialize the KeyFrameDiskWriter to save keyframes at the desired location\n",
        "    diskwriter = KeyFrameDiskWriter(location=f\"frames\")\n",
        "\n",
        "    # Iterate through all files in the downloads folder for the given batch ID\n",
        "    for filename in os.listdir(f'downloads{id}'):\n",
        "        # Construct the full path to the video file\n",
        "        video_file_path = f\"downloads{id}/{filename}\"\n",
        "\n",
        "        # Extract keyframes and process the data using the diskwriter\n",
        "        try:\n",
        "            vd.extract_video_keyframes(\n",
        "                no_of_frames=no_of_frames_to_returned,\n",
        "                file_path=video_file_path,\n",
        "                writer=diskwriter\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # Skip processing this file if an exception occurs\n",
        "            continue\n"
      ],
      "metadata": {
        "id": "iGOi_-t281zb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import numpy as np\n",
        "\n",
        "mkdir('audio')\n",
        "df = pd.read_csv(csv_path)\n",
        "timestamps = get_timestamps(df)\n",
        "n_chunks = len(df)/100  # More chunks than workers\n",
        "n_workers = os.cpu_count() + 4 # Limited number of workers\n",
        "\n",
        "# Split the DataFrame into chunks\n",
        "chunks = np.array_split(df, n_chunks)\n",
        "\n",
        "def process_chunk(chunk, id):\n",
        "    yt_ids = chunk['youtube_id']\n",
        "    download_column(yt_ids, id)\n",
        "    get_clips(yt_ids, id)\n",
        "    get_audio(id)\n",
        "    get_frames(id)\n",
        "    command = ['rm', '-r', f'downloads{id}']\n",
        "    subprocess.run(command)\n",
        "\n",
        "# Use ThreadPoolExecutor with limited workers\n",
        "with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
        "    futures = [executor.submit(process_chunk, chunk, i) for i, chunk in enumerate(chunks)]\n",
        "    for future in as_completed(futures):\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing chunk: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cKqEl6Vm0aq",
        "outputId": "9b4fc6bd-b4cf-429f-8400-f658c5cdd449",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=--4gqARaEJE\n",
            "[youtube] --4gqARaEJE: Downloading webpage\n",
            "[youtube] --4gqARaEJE: Downloading ios player API JSON\n",
            "[youtube] --4gqARaEJE: Downloading mweb player API JSON\n",
            "[youtube] --4gqARaEJE: Downloading player 5b77d519\n",
            "[youtube] --4gqARaEJE: Downloading m3u8 information\n",
            "[info] --4gqARaEJE: Downloading 1 format(s): 136+251\n",
            "[download] Destination: downloads0/Miniature, Standard, Teacup Dachshund, Puppies, For, Sale, In, New Jersey, NJ, PA, DE, MD,CT [--4gqARaEJE].f136.mp4\n",
            "[download] 100% of    7.92MiB in 00:00:00 at 17.20MiB/s  \n",
            "[download] Destination: downloads0/Miniature, Standard, Teacup Dachshund, Puppies, For, Sale, In, New Jersey, NJ, PA, DE, MD,CT [--4gqARaEJE].f251.webm\n",
            "[download] 100% of  774.71KiB in 00:00:00 at 8.61MiB/s   \n",
            "[Merger] Merging formats into \"downloads0/Miniature, Standard, Teacup Dachshund, Puppies, For, Sale, In, New Jersey, NJ, PA, DE, MD,CT [--4gqARaEJE].mkv\"\n",
            "Deleting original file downloads0/Miniature, Standard, Teacup Dachshund, Puppies, For, Sale, In, New Jersey, NJ, PA, DE, MD,CT [--4gqARaEJE].f251.webm (pass -k to keep)\n",
            "Deleting original file downloads0/Miniature, Standard, Teacup Dachshund, Puppies, For, Sale, In, New Jersey, NJ, PA, DE, MD,CT [--4gqARaEJE].f136.mp4 (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=--BfvyPmVMo\n",
            "[youtube] --BfvyPmVMo: Downloading webpage\n",
            "[youtube] --BfvyPmVMo: Downloading ios player API JSON\n",
            "[youtube] --BfvyPmVMo: Downloading mweb player API JSON\n",
            "[youtube] --BfvyPmVMo: Downloading m3u8 information\n",
            "[info] --BfvyPmVMo: Downloading 1 format(s): 135+140\n",
            "[download] Destination: downloads0/Vehicle Alignments [--BfvyPmVMo].f135.mp4\n",
            "[download] 100% of   10.79MiB in 00:00:00 at 15.83MiB/s  \n",
            "[download] Destination: downloads0/Vehicle Alignments [--BfvyPmVMo].f140.m4a\n",
            "[download] 100% of    3.12MiB in 00:00:00 at 16.70MiB/s  \n",
            "[Merger] Merging formats into \"downloads0/Vehicle Alignments [--BfvyPmVMo].mp4\"\n",
            "Deleting original file downloads0/Vehicle Alignments [--BfvyPmVMo].f135.mp4 (pass -k to keep)\n",
            "Deleting original file downloads0/Vehicle Alignments [--BfvyPmVMo].f140.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=--U7joUcTCo\n",
            "[youtube] --U7joUcTCo: Downloading webpage\n",
            "[youtube] --U7joUcTCo: Downloading ios player API JSON\n",
            "[youtube] --U7joUcTCo: Downloading mweb player API JSON\n",
            "[youtube] --U7joUcTCo: Downloading m3u8 information\n",
            "[info] --U7joUcTCo: Downloading 1 format(s): 136+140\n",
            "[download] Destination: downloads0/VELHO DA TOSSE.wmv [--U7joUcTCo].f136.mp4\n",
            "[download] 100% of    1.06MiB in 00:00:00 at 3.15MiB/s   \n",
            "[download] Destination: downloads0/VELHO DA TOSSE.wmv [--U7joUcTCo].f140.m4a\n",
            "[download] 100% of  147.08KiB in 00:00:00 at 1.44MiB/s   \n",
            "[Merger] Merging formats into \"downloads0/VELHO DA TOSSE.wmv [--U7joUcTCo].mp4\"\n",
            "Deleting original file downloads0/VELHO DA TOSSE.wmv [--U7joUcTCo].f136.mp4 (pass -k to keep)\n",
            "Deleting original file downloads0/VELHO DA TOSSE.wmv [--U7joUcTCo].f140.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-0BIyqJj9ZU\n",
            "[youtube] -0BIyqJj9ZU: Downloading webpage\n",
            "[youtube] -0BIyqJj9ZU: Downloading ios player API JSON\n",
            "[youtube] -0BIyqJj9ZU: Downloading mweb player API JSON\n",
            "[youtube] -0BIyqJj9ZU: Downloading m3u8 information\n",
            "[info] -0BIyqJj9ZU: Downloading 1 format(s): 18\n",
            "[download] Destination: downloads0/Risate a catena (Skype Laughter Chain) [-0BIyqJj9ZU].mp4\n",
            "[download] 100% of    9.84MiB in 00:00:00 at 14.34MiB/s  \n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-0RWZT-miFs\n",
            "[youtube] -0RWZT-miFs: Downloading webpage\n",
            "[youtube] -0RWZT-miFs: Downloading ios player API JSON\n",
            "[youtube] -0RWZT-miFs: Downloading mweb player API JSON\n",
            "[youtube] -0RWZT-miFs: Downloading m3u8 information\n",
            "[info] -0RWZT-miFs: Downloading 1 format(s): 136+140\n",
            "[download] Destination: downloads0/ONE LAST SPECIAL GIFT!! [-0RWZT-miFs].f136.mp4\n",
            "[download] 100% of  151.55MiB in 00:00:04 at 35.03MiB/s  \n",
            "[download] Destination: downloads0/ONE LAST SPECIAL GIFT!! [-0RWZT-miFs].f140.m4a\n",
            "[download] 100% of   11.86MiB in 00:00:00 at 25.79MiB/s  \n",
            "[Merger] Merging formats into \"downloads0/ONE LAST SPECIAL GIFT!! [-0RWZT-miFs].mp4\"\n",
            "Deleting original file downloads0/ONE LAST SPECIAL GIFT!! [-0RWZT-miFs].f140.m4a (pass -k to keep)\n",
            "Deleting original file downloads0/ONE LAST SPECIAL GIFT!! [-0RWZT-miFs].f136.mp4 (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-0nqfRcnAYE\n",
            "[youtube] -0nqfRcnAYE: Downloading webpage\n",
            "[youtube] -0nqfRcnAYE: Downloading ios player API JSON\n",
            "[youtube] -0nqfRcnAYE: Downloading mweb player API JSON\n",
            "[youtube] -0nqfRcnAYE: Downloading m3u8 information\n",
            "[info] -0nqfRcnAYE: Downloading 1 format(s): 135+140\n",
            "[download] Destination: downloads0/Come Mettere La Matita Nera [-0nqfRcnAYE].f135.mp4\n",
            "[download] 100% of   22.40MiB in 00:00:00 at 26.43MiB/s  \n",
            "[download] Destination: downloads0/Come Mettere La Matita Nera [-0nqfRcnAYE].f140.m4a\n",
            "[download] 100% of   10.18MiB in 00:00:00 at 22.31MiB/s  \n",
            "[Merger] Merging formats into \"downloads0/Come Mettere La Matita Nera [-0nqfRcnAYE].mp4\"\n",
            "Deleting original file downloads0/Come Mettere La Matita Nera [-0nqfRcnAYE].f140.m4a (pass -k to keep)\n",
            "Deleting original file downloads0/Come Mettere La Matita Nera [-0nqfRcnAYE].f135.mp4 (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-116CjQ3MAg\n",
            "[youtube] -116CjQ3MAg: Downloading webpage\n",
            "[youtube] -116CjQ3MAg: Downloading ios player API JSON\n",
            "[youtube] -116CjQ3MAg: Downloading mweb player API JSON\n",
            "[youtube] -116CjQ3MAg: Downloading m3u8 information\n",
            "[info] -116CjQ3MAg: Downloading 1 format(s): 18\n",
            "[download] Destination: downloads0/Hawk Attack Kill Eat Swoop Mockingbird Frog wild [-116CjQ3MAg].mp4\n",
            "[download] 100% of    6.02MiB in 00:00:00 at 9.03MiB/s   \n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-1EXhfqLLwQ\n",
            "[youtube] -1EXhfqLLwQ: Downloading webpage\n",
            "[youtube] -1EXhfqLLwQ: Downloading ios player API JSON\n",
            "[youtube] -1EXhfqLLwQ: Downloading mweb player API JSON\n",
            "[youtube] -1EXhfqLLwQ: Downloading m3u8 information\n",
            "[info] -1EXhfqLLwQ: Downloading 1 format(s): 135+140\n",
            "[download] Destination: downloads0/Doberman in bathtub [-1EXhfqLLwQ].f135.mp4\n",
            "[download] 100% of   14.37MiB in 00:00:00 at 23.08MiB/s  \n",
            "[download] Destination: downloads0/Doberman in bathtub [-1EXhfqLLwQ].f140.m4a\n",
            "[download] 100% of    2.63MiB in 00:00:00 at 27.99MiB/s  \n",
            "[Merger] Merging formats into \"downloads0/Doberman in bathtub [-1EXhfqLLwQ].mp4\"\n",
            "Deleting original file downloads0/Doberman in bathtub [-1EXhfqLLwQ].f135.mp4 (pass -k to keep)\n",
            "Deleting original file downloads0/Doberman in bathtub [-1EXhfqLLwQ].f140.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-1PZQg5Gi8A\n",
            "[youtube] -1PZQg5Gi8A: Downloading webpage\n",
            "[youtube] -1PZQg5Gi8A: Downloading ios player API JSON\n",
            "[youtube] -1PZQg5Gi8A: Downloading mweb player API JSON\n",
            "[youtube] -1PZQg5Gi8A: Downloading m3u8 information\n",
            "[info] -1PZQg5Gi8A: Downloading 1 format(s): 18\n",
            "[download] Destination: downloads0/Smash Bros Lawl Moveset-CD-I Link [-1PZQg5Gi8A].mp4\n",
            "[download] 100% of   16.72MiB in 00:00:01 at 13.57MiB/s  \n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=-1pPw9zZopA\n",
            "[youtube] -1pPw9zZopA: Downloading webpage\n",
            "[youtube] -1pPw9zZopA: Downloading ios player API JSON\n",
            "[youtube] -1pPw9zZopA: Downloading mweb player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: [youtube] -1pPw9zZopA: Video unavailable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed processing for :  downloads0/Come Mettere La Matita Nera [-0nqfRcnAYE]_cut.mp4\n",
            "Completed processing for :  downloads0/VELHO DA TOSSE.wmv [--U7joUcTCo]_cut.mp4\n",
            "Completed processing for :  downloads0/ONE LAST SPECIAL GIFT!! [-0RWZT-miFs]_cut.mp4\n",
            "Completed processing for :  downloads0/Hawk Attack Kill Eat Swoop Mockingbird Frog wild [-116CjQ3MAg]_cut.mp4\n",
            "Completed processing for :  downloads0/Risate a catena (Skype Laughter Chain) [-0BIyqJj9ZU]_cut.mp4\n",
            "Completed processing for :  downloads0/Vehicle Alignments [--BfvyPmVMo]_cut.mp4\n",
            "Completed processing for :  downloads0/Doberman in bathtub [-1EXhfqLLwQ]_cut.mp4\n",
            "Completed processing for :  downloads0/Smash Bros Lawl Moveset-CD-I Link [-1PZQg5Gi8A]_cut.mp4\n"
          ]
        }
      ]
    }
  ]
}